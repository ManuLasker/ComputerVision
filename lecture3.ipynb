{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lost function Support vector machines:\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, S_j - S_{y_i} + 1)$\n",
    "\n",
    "Note that in this particular module we are working with linear score functions ($f(x_i;W) = Wx_i$), so we can also rewrite the loss function in this equivalent form:\n",
    "\n",
    "$L_i = \\sum_{j\\neq y_i} max(0, w^{T}_{j}x_i - w^{T}_{y_i}x_i  + 1)$\n",
    "\n",
    "where $w_{j}$ is the j-th row of W reshaped as a column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lost function with regularization penalty\n",
    "\n",
    "$L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss}$\n",
    "\n",
    "Or expanding this out in its full form:\n",
    "\n",
    "$L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_i_vectorized(x, y, W):\n",
    "  \"\"\"\n",
    "  A faster half-vectorized implementation. half-vectorized\n",
    "  refers to the fact that for a single example the implementation contains\n",
    "  no for loops, but there is still one loop over the examples (outside this function)\n",
    "  \"\"\"\n",
    "  delta = 1.0\n",
    "  scores = W.dot(x)\n",
    "  # compute the margins for all classes in one vector operation\n",
    "  margins = np.maximum(0, scores - scores[y] + delta)\n",
    "  # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "  # to ignore the y-th position and only consider margin on max wrong class\n",
    "  margins[y] = 0\n",
    "  loss_i = np.sum(margins)\n",
    "  return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent optimization:\n",
    "$L(W) = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2$\n",
    "\n",
    "let's just derivate the lost function respect to W\n",
    "$\\nabla_{w_j}L(W) = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\nabla_{w_j}\\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\nabla_{w_j}\\sum_k\\sum_l W_{k,l}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "while True:\n",
    "  weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "  weights += - step_size * weights_grad # perform parameter update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batch gradient descent\n",
    "\n",
    "A very common approach to addressing this challenge is to compute the gradient over batches of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "\n",
    "while True:\n",
    "  data_batch = sample_training_data(data, 256) # sample 256 examples\n",
    "  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "  weights += - step_size * weights_grad # perform parameter update\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
